# On Evaluating the Efficiency of Source Code Generated by LLMs

## Referência Completa
C. Niu, T. Zhang, C. Li, B. Luo and V. Ng, "On Evaluating the Efficiency of Source Code Generated by LLMs," 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:, Lisbon, Portugal, 2024, pp. 103-107, doi: 10.1145/3650105.3652295.

## 1. Fichamento de Conteúdo

O artigo investiga a eficiência do código gerado por modelos de linguagem de grande escala (LLMs), indo além da mera avaliação da correção funcional. Os autores conduzem experimentos utilizando os benchmarks HumanEval e MBPP, além de problemas do LeetCode para avaliar a execução e desempenho dos códigos gerados. Eles exploram a eficácia de diferentes prompts para melhorar a eficiência do código gerado, comparando modelos como GPT-4, GPT-3.5, Code Llama, WizardCoder e DeepSeek Coder. Os resultados indicam que a eficiência não está necessariamente correlacionada à taxa de acerto do modelo e que estratégias de prompt, como decomposição passo a passo, podem melhorar significativamente o desempenho do código gerado. O estudo destaca que modelos menores podem gerar código mais eficiente que modelos maiores em certas situações e que a escolha do modelo deve considerar o equilíbrio entre correção e eficiência.

## 2. Fichamento Bibliográfico

- _Eficiência do Código Gerado por LLMs_: Mede o tempo de execução dos códigos gerados por modelos de IA, além da correção funcional.
- _Benchmarks Utilizados_: HumanEval e MBPP para testes de programação de nível básico, além do LeetCodeEval para problemas mais complexos.
- _Pass@10_: Métrica que avalia a taxa de sucesso de um modelo ao gerar pelo menos uma solução correta entre as 10 primeiras tentativas.
- _Estratégias de Prompt_: Métodos para incentivar modelos a gerar código mais eficiente, incluindo otimização direta, otimização iterativa e análise de estratégia antes da geração do código final.
- _Modelos Avaliados_: GPT-4, GPT-3.5, Code Llama, WizardCoder e DeepSeek Coder foram testados quanto à eficiência e correção do código gerado.

## 3. Fichamento de Citações

- "_The ability to generate correct code is not positively correlated with the ability to generate efficient code._"
- "_Step-by-step prompting could make LLMs generate more efficient code, especially on complex problems._"
- "_We propose a LeetCode-based benchmark which provides a reference point for comparing the correctness and efficiency of more complex code._"
- "_Our results suggest that models of varying sizes share similar performance due to their reliance on the same training data._"
- "_More comprehensive test cases can make the runtime benefits of code with lower complexity more significant, and thus more accurately reflect efficiency._"
- "_Prompting strategies such as chain-of-thought improve efficiency for complex problems, as they allow the model to iteratively refine its output._"
